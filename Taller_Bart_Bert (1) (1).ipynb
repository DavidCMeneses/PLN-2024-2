{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Estudiante: David Esteban Casallas Meneses"
      ],
      "metadata": {
        "id": "GEo8sumia7pa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Taller\n",
        "\n",
        "En este taller se utilizaran dos herramientas para lograr resumenes BERT de Google y BART de Facebook, el objetivo es compararlas modificando los atributos base.\n",
        "\n",
        "*   Buscar un texto de mínimo 1000 palabras y modificarlo en la variable BODY\n",
        "*   Modificar los atributos y verificar los resultados, luego hacer el mismo ejercicio con una herramienta en línea tipo CHAT GPT, Claude, Copilot ETC, comparar los resultados.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# result = model(body, ratio=0.2, #This parameter sets the length of the summary relative to the original text. In this case, it aims for a summary that is number% the length of the original text.\n",
        "               min_length=60, #This parameter specifies the minimum number of words or characters (depending on how the summarizer is implemented) that the summary should have.\n",
        "               num_sentences=3) #This parameter limits the summary to a maximum of number sentences. The output of the summarization process is stored in the result variable. It likely contains the summarized text.\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# summary_ids = model.generate(article_input_ids,\n",
        "                             num_beams=4, #Sets the number of beams for beam search, a decoding strategy that explores multiple possibilities to find a better summary.\n",
        "                             length_penalty=2.0, # Discourages very long summaries by penalizing them.\n",
        "                             max_length=300, # Limits the summary to a maximum of tokens.\n",
        "                             min_length=200, # Ensures the summary has at least tokens\n",
        "                             no_repeat_ngram_size=3) #Prevents the model from repeating phrases of 3 words or more.\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> (BART) can be seen as generalizing Bert (due to the bidirectional encoder)\n"
      ],
      "metadata": {
        "id": "hgAvDQE3C8cN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BODY = \"\"\"\n",
        "Prologue\n",
        "\n",
        "Mystery Man\n",
        "\n",
        "I felt the covers slide down my body then a hand light on the small of my back. It was so warm it was hot, like the blood that ran through its veins went faster than the blood of any average man.\n",
        "\n",
        "If this was true, it wouldn’t surprise me.\n",
        "\n",
        "I opened my eyes and it was dark. It was always dark when he visited me.\n",
        "\n",
        "I had a moment like every moment I had when he showed. A moment of sanity. A moment where my mind said to close my eyes and open my mouth and tell him to go away.\n",
        "\n",
        "But if I did, I knew he would. He wouldn’t say a word. As silently as he came, he’d leave.\n",
        "\n",
        "And he’d never come back.\n",
        "\n",
        "But this was the right thing to do. The smart thing to do. The sane thing to do.\n",
        "\n",
        "And I was thinking of doing it, honest to God, I was. I thought about doing it every time.\n",
        "\n",
        "Then I felt his weight hit the bed, his body stretching out beside mine, he turned me into him, I opened my mouth to speak and before I could do the sane thing, his mouth was on mine.\n",
        "\n",
        "And for the next two hours, I didn’t think at all.\n",
        "\n",
        "But I felt. I felt a lot.\n",
        "\n",
        "And all of it was good.\n",
        "\n",
        "It was still dark when his shadow moved in the room.\n",
        "\n",
        "I lay in bed and watched him move. He didn’t make a noise. It was weird. There was a rustle of clothes but other than that, silence.\n",
        "\n",
        "Even as a shadow, I saw he had masculine grace. Powerful masculine grace. That was weird too. Just my mystery man putting on clothes was like watching a badass, macho dance if there was such a thing. Of course, there wasn’t except in my bedroom when he came to visit. No, when he was getting ready to leave.\n",
        "\n",
        "It was so fascinating I should sell tickets. But if I did, I’d have to share. I probably already shared with half of Denver, all of them getting their own private show. That already messed with my head enough, that and the fact that he came at all, I let him come, then he made me come after which he came. Then, often, like tonight, repeat.\n",
        "\n",
        "I wasn’t real hot on sharing any more than I already likely did.\n",
        "\n",
        "He moved to the bed and I watched that too. He bent low, I felt the heat of his hand on my knee, his fingers curling around the back and he lightly kissed my hip, his lips skimming across my skin, making it tingle. Then he slid the covers up my body to my waist where he dropped them.\n",
        "\n",
        "I was mostly on my belly, partly on my side, my arm crooked, hand tucked under my face on the pillow. His body moved in that direction, his fingers slid under my hair, pulling it gently back and his lips came to my ear.\n",
        "\n",
        "“Later, babe,” he whispered.\n",
        "\n",
        "“Later,” I whispered back.\n",
        "\n",
        "His head moved infinitesimally and his lips skimmed the skin at the back of my ear then his tongue touched there. That made my skin tingle too, so much my whole body shivered.\n",
        "\n",
        "He pulled the covers up to my shoulder.\n",
        "\n",
        "Then he turned and he was gone.\n",
        "\n",
        "No noise, not even the door opening and closing. He was just gone. Like he’d never even been there.\n",
        "\n",
        "Freaking crazy.\n",
        "\n",
        "I stared at my bedroom door awhile. My body felt warm, sated and tired. My mind did not feel the same.\n",
        "\n",
        "I turned to my back, tucked the covers around my na**d body and I stared at the ceiling.\n",
        "\n",
        "I didn’t even know his name.\n",
        "\n",
        "“God,” I whispered, “I am such a slut.”\n",
        "\n",
        "Chapter One\n",
        "\n",
        "D-e-a-d, Dead\n",
        "\n",
        "The next morning I was sitting at my computer in my home office.\n",
        "\n",
        "I should have been working. I had three deadlines the next two weeks and I’d barely begun on the work. I was a freelance editor. I got paid by the hour and if I didn’t work that hour, I didn’t get paid. I had a mouth to feed, my own. I had a body to clothe, a body that liked all sorts of clothes, it craved them so I had to feed the habit or things could get nasty. I had a cosmopolitan addiction and cosmos didn’t come cheap. And I had a house I was fixing up. Therefore, I needed to get paid.\n",
        "\n",
        "Okay, that wasn’t strictly true. I wasn’t fixing up my house. My Dad did some of the work. My friend Troy did other work. So, I should say that I had a house I was guilting, begging and emotionally blackmailing others into fixing up.\n",
        "\n",
        "But still, it needed fixing up and cabinets and tile didn’t march from Cabinet and Tile Land into my house and say, “We want to live with you, Gwendolyn Kidd, fix us to your walls!”\n",
        "\n",
        "That only happened in my dreams, of which I had many, most of them daydreams.\n",
        "\n",
        "Like right then, sitting at my computer, one heel to the seat, my chin to my knee, my eyes staring out the window, I was thinking about my Mystery Man, the Great MM. I was daydreaming about changing our first meeting. Being smarter, funnier, more mysterious, alluring, interesting, hooking him instantly with my rapier wit, my flair for conversation, my ability to discuss politics and world events intelligently, my humble stories of expansive charity work all wrapped up with enticing looks that promised a lifetime of mind-blowing orgasms, making him declare his undying love for me.\n",
        "\n",
        "Or at least tell me his name.\n",
        "\n",
        "Instead, I was drunk and definitely not any of that.\n",
        "\n",
        "I heard my doorbell go, a chime then a clunk and I started out of my elaborate daydream which was beginning to get good.\n",
        "\n",
        "Then I got up and walked through my office into the upstairs hall making a mental note, again, to call Troy and see if he’d fix my doorbell for a six pack and a homemade pizza. This might mean he’d bring his annoying, whiny, constantly bitching new girlfriend though, so I changed my mind and decided to call my Dad.\n",
        "\n",
        "I got to the bottom of my stairs and walked through my wide living room, ignoring the state of it, which was decorated in Fix Up Chic, in other words dust rags, paint brushes, power tools, not-so-power-tools, cans and tubes of practically everything, all of it jumbled and covered in a layer of dust. I made it through the area without my hands going to my head, fingers clenching my hair and mouth screaming, which I counted as progress.\n",
        "\n",
        "I got to the entryway which was delineated by two narrow walls both fit with gorgeous stained glass.\n",
        "\n",
        "Two years ago, that stained glass was my undoing.\n",
        "\n",
        "Two years ago, approximately six months and two weeks prior to meeting my Mystery Man, I’d walked one single step into this ramble and wreck of a house, saw that stained glass, turned to the realtor and announced, “I’ll take it.”\n",
        "\n",
        "The realtor’s face had lit up.\n",
        "\n",
        "My father, who hadn’t even made it into the house yet, turned his eyes to the heavens. His prayer lasted a long time. His lecture longer.\n",
        "\n",
        "I still bought the house.\n",
        "\n",
        "As usual, I should have listened to my Dad.\n",
        "\"\"\".replace('\\n','')"
      ],
      "metadata": {
        "id": "L-ypy-WvLqsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modelo BERT"
      ],
      "metadata": {
        "id": "uM8uXY2BOWUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-extractive-summarizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiVHAeXrL9mH",
        "outputId": "6c41b159-4ebd-4bc1-cd4e-105c942ff7f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bert-extractive-summarizer\n",
            "  Downloading bert_extractive_summarizer-0.10.1-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from bert-extractive-summarizer) (4.47.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from bert-extractive-summarizer) (1.6.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (from bert-extractive-summarizer) (3.7.5)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->bert-extractive-summarizer) (3.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer) (2.10.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers->bert-extractive-summarizer) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers->bert-extractive-summarizer) (0.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->bert-extractive-summarizer) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->bert-extractive-summarizer) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->bert-extractive-summarizer) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers->bert-extractive-summarizer) (0.5.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers->bert-extractive-summarizer) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers->bert-extractive-summarizer) (4.12.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->bert-extractive-summarizer) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->bert-extractive-summarizer) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->bert-extractive-summarizer) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->bert-extractive-summarizer) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->bert-extractive-summarizer) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy->bert-extractive-summarizer) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy->bert-extractive-summarizer) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy->bert-extractive-summarizer) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->bert-extractive-summarizer) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->bert-extractive-summarizer) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy->bert-extractive-summarizer) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->bert-extractive-summarizer) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->bert-extractive-summarizer) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->bert-extractive-summarizer) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->bert-extractive-summarizer) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->bert-extractive-summarizer) (0.1.2)\n",
            "Downloading bert_extractive_summarizer-0.10.1-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: bert-extractive-summarizer\n",
            "Successfully installed bert-extractive-summarizer-0.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from summarizer import Summarizer"
      ],
      "metadata": {
        "id": "jyA35gbALkLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Summarizer()\n",
        "result = model(BODY, ratio=0.2, #This parameter sets the length of the summary relative to the original text. In this case, it aims for a summary that is number% the length of the original text.\n",
        "               min_length=60, #This parameter specifies the minimum number of words or characters (depending on how the summarizer is implemented) that the summary should have.\n",
        "               num_sentences=3) #This parameter limits the summary to a maximum of number sentences. The output of the summarization process is stored in the result variable. It likely contains the summarized text.\n",
        "full = ''.join(result)\n",
        "print(full)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2v5nbYJMLlWb",
        "outputId": "ba86e942-c700-4efc-9c08-68766d639868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PrologueMystery ManI felt the covers slide down my body then a hand light on the small of my back. I probably already shared with half of Denver, all of them getting their own private show. I got to the bottom of my stairs and walked through my wide living room, ignoring the state of it, which was decorated in Fix Up Chic, in other words dust rags, paint brushes, power tools, not-so-power-tools, cans and tubes of practically everything, all of it jumbled and covered in a layer of dust.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Summarizer()\n",
        "result = model(BODY, ratio=0.3, #This parameter sets the length of the summary relative to the original text. In this case, it aims for a summary that is number% the length of the original text.\n",
        "               min_length=50, #This parameter specifies the minimum number of words or characters (depending on how the summarizer is implemented) that the summary should have.\n",
        "               num_sentences=5) #This parameter limits the summary to a maximum of number sentences. The output of the summarization process is stored in the result variable. It likely contains the summarized text.\n",
        "full = ''.join(result)\n",
        "print(full)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzUs6bBfXyfj",
        "outputId": "2364dc0d-ce64-42fb-af7b-e2cb209d01f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PrologueMystery ManI felt the covers slide down my body then a hand light on the small of my back. I turned to my back, tucked the covers around my na**d body and I stared at the ceiling. I was daydreaming about changing our first meeting. Instead, I was drunk and definitely not any of that. I got to the bottom of my stairs and walked through my wide living room, ignoring the state of it, which was decorated in Fix Up Chic, in other words dust rags, paint brushes, power tools, not-so-power-tools, cans and tubes of practically everything, all of it jumbled and covered in a layer of dust.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Summarizer()\n",
        "result = model(BODY, ratio=0.1, #This parameter sets the length of the summary relative to the original text. In this case, it aims for a summary that is number% the length of the original text.\n",
        "               min_length=50, #This parameter specifies the minimum number of words or characters (depending on how the summarizer is implemented) that the summary should have.\n",
        "               num_sentences=3) #This parameter limits the summary to a maximum of number sentences. The output of the summarization process is stored in the result variable. It likely contains the summarized text.\n",
        "full = ''.join(result)\n",
        "print(full)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJIJNxlCX472",
        "outputId": "ebcbd431-be95-4813-fcfe-3ddde221050a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PrologueMystery ManI felt the covers slide down my body then a hand light on the small of my back. I probably already shared with half of Denver, all of them getting their own private show. I got to the bottom of my stairs and walked through my wide living room, ignoring the state of it, which was decorated in Fix Up Chic, in other words dust rags, paint brushes, power tools, not-so-power-tools, cans and tubes of practically everything, all of it jumbled and covered in a layer of dust.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Summarizer()\n",
        "result = model(BODY, ratio=0.1, #This parameter sets the length of the summary relative to the original text. In this case, it aims for a summary that is number% the length of the original text.\n",
        "               min_length=10, #This parameter specifies the minimum number of words or characters (depending on how the summarizer is implemented) that the summary should have.\n",
        "               num_sentences=1) #This parameter limits the summary to a maximum of number sentences. The output of the summarization process is stored in the result variable. It likely contains the summarized text.\n",
        "full = ''.join(result)\n",
        "print(full)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cARcGQ1jYUK1",
        "outputId": "1407a2f7-75a3-42ab-f546-ad1953615acd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PrologueMystery ManI felt the covers slide down my body then a hand light on the small of my back.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo BART"
      ],
      "metadata": {
        "id": "li835NwDOfEP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81OSvG4DvxlZ",
        "outputId": "2a3e6a0e-c34b-41a0-e2a9-19d88eabaea3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "#collapse-hide\n",
        "INSTALL_MSG = \"\"\"\n",
        "Bart will be released through pip in v 3.0.0, until then use it by installing from source:\n",
        "\n",
        "git clone git@github.com:huggingface/transformers.git\n",
        "git checkout d6de6423\n",
        "cd transformers\n",
        "pip install -e \".[dev]\"\n",
        "\n",
        "\"\"\"\n",
        "!pip install transformers\n",
        "import torch\n",
        "try:\n",
        "    import transformers\n",
        "    from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "except ImportError:\n",
        "    raise ImportError(INSTALL_MSG)\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn') # Changed model identifier\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn') # Changed model identifier\n",
        "\n",
        "article_input_ids = tokenizer.batch_encode_plus([BODY], return_tensors='pt', max_length=1024)['input_ids'].to(torch_device)\n",
        "summary_ids = model.generate(article_input_ids,\n",
        "                             num_beams=4, #Sets the number of beams for beam search, a decoding strategy that explores multiple possibilities to find a better summary.\n",
        "                             length_penalty=2.0, # Discourages very long summaries by penalizing them.\n",
        "                             max_length=300, # Limits the summary to a maximum of tokens.\n",
        "                             min_length=200, # Ensures the summary has at least tokens\n",
        "                             no_repeat_ngram_size=3) #Prevents the model from repeating phrases of 3 words or more.\n",
        "\n",
        "summary_txt = tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
        "display(Markdown('> **Summary: **'+summary_txt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "dwXOulxEwAUo",
        "outputId": "b84d931c-20b0-457a-da6c-a7113841b36c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **Summary: **Mystery Man is the story of a woman who falls in love with a mysterious man. The story follows her as she tries to come to terms with her feelings for her mystery man. She also finds out that the man is not who he seems to be and that he is not the man she thought he was. The book is the first in a series of three books about the same man, called D-e-a-d, which is about a man who falls for a woman and goes on to have sex with her several times a week. It is written in the style of a young adult novel, with a focus on women and sex. The author is also a freelance editor and has written a book about her own experiences with men, called The Man Who Fell For A Woman, which you can read at the end of the book. For more information on the book, or to order your own copy, visit the publisher’s website or go to: http://www.the-man-who-fell-for-me-and-the-book.com/."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn') # Changed model identifier\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn') # Changed model identifier\n",
        "\n",
        "article_input_ids = tokenizer.batch_encode_plus([BODY], return_tensors='pt', max_length=1024)['input_ids'].to(torch_device)\n",
        "summary_ids = model.generate(article_input_ids,\n",
        "                             num_beams=8, #Sets the number of beams for beam search, a decoding strategy that explores multiple possibilities to find a better summary.\n",
        "                             length_penalty=5.0, # Discourages very long summaries by penalizing them.\n",
        "                             max_length=200, # Limits the summary to a maximum of tokens.\n",
        "                             min_length=100, # Ensures the summary has at least tokens\n",
        "                             no_repeat_ngram_size=3) #Prevents the model from repeating phrases of 3 words or more.\n",
        "\n",
        "summary_txt = tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
        "display(Markdown('> **Summary: **'+summary_txt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "qQTIBa6yYmXv",
        "outputId": "3de58d1b-9eb2-41ad-dce2-0491ce3a1b55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **Summary: **D-e-a-d is the first book in a series about a mystery man. The story follows the author as she tries to come to terms with her relationship with the man she calls her \"mystery man\" The book is set in Denver, Colorado, in the late 1990s and early 2000s. It is the story of a woman who falls in love with her mystery man and finds out that he is not who he seems to be. The book ends with the author dead and the mystery man alive and well."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a comparar entonces estos resultados con un modelo creado usando Chatgpt"
      ],
      "metadata": {
        "id": "s6cpATfTZQhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMbNznOdWQWs",
        "outputId": "9221f410-5ef2-4143-aa43-a9c140c6c47b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "def summarize_text(body, max_length=130, min_length=30, length_penalty=2.0):\n",
        "    \"\"\"\n",
        "    Genera un resumen de texto usando un modelo preentrenado de Hugging Face.\n",
        "\n",
        "    Parámetros:\n",
        "        body (str): El texto completo que se desea resumir.\n",
        "        max_length (int): Longitud máxima del resumen.\n",
        "        min_length (int): Longitud mínima del resumen.\n",
        "        length_penalty (float): Penalización para la longitud del resumen (valores mayores generan resúmenes más cortos).\n",
        "\n",
        "    Retorna:\n",
        "        str: El texto resumido.\n",
        "    \"\"\"\n",
        "    # Cargar el modelo y el tokenizer preentrenados\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "    # Tokenizar el texto de entrada\n",
        "    inputs = tokenizer.encode(\"summarize: \" + body, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "\n",
        "    # Generar el resumen\n",
        "    summary_ids = model.generate(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        length_penalty=length_penalty,\n",
        "        num_beams=4,  # Beam search para mejorar la calidad\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decodificar el resumen generado\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Llamar a la función para generar el resumen\n",
        "resumen = summarize_text(BODY)\n",
        "print(\"Resumen generado:\")\n",
        "print(resumen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCfhlO1dZX99",
        "outputId": "e35043e7-b751-49f8-8063-9f4ee8528575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resumen generado:\n",
            "Mystery Man is the story of a woman who falls in love with a man she doesn't even know. The story follows her as she tries to come to terms with her feelings for the man she loves.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y adicionalmente le daré el texto a chatgpt para pedirle que me dé un resumen."
      ],
      "metadata": {
        "id": "383EhY7XZnTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resumen del texto:\n",
        "\n",
        "En el prólogo, la protagonista narra sus encuentros nocturnos con un hombre misterioso al que no conoce ni siquiera por su nombre. Aunque sabe que lo más sensato sería rechazarlo, no puede resistirlo. Él aparece silenciosamente, y sus visitas están llenas de pasión y conexión física. Sin embargo, siempre se va de la misma forma: en silencio, como si nunca hubiera estado allí, dejando a la protagonista reflexionando sobre su situación y llamándose a sí misma “una mujer fácil”.\n",
        "\n",
        "En el primer capítulo, la protagonista, Gwendolyn Kidd, está en su casa lidiando con su vida diaria como editora freelance. A pesar de tener múltiples responsabilidades, como mantener su casa en proceso de renovación y sostener su adicción a los cosmopolitan, su mente sigue divagando hacia el hombre misterioso. A través de su humor autocrítico y las distracciones de su desordenada casa, se muestra que su vida personal es un caos controlado. El capítulo concluye con Gwendolyn escuchando el timbre de su puerta, preparándose para una posible interrupción en su día."
      ],
      "metadata": {
        "id": "SRMyoaPAZtNK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En resumen para concluir el primer modelo parace únicamente capaz de repetir o mezclar fragmentos del texto que le pasamos, mientras que en el segundo modelo ya hay un texto con sentido y formulado como un \"resumen\" no una mezcla de fragmentos. Chatgpt generó un código identico al segundo método proporcionado y sin embargo su versión al ser más corta le dió menos libertad al modelo para divagar. Finalmente el propio Chatgpt es capaz de entrar en detalle en el resumen pero hay que tener en cuenta que él"
      ],
      "metadata": {
        "id": "jjuJj3AuaJ93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referencia\n",
        "https://github.com/sshleifer/blog_v2/blob/master/_notebooks/2020-03-12-bart.ipynb\n",
        "\n",
        "https://github.com/dmmiller612/bert-extractive-summarizer"
      ],
      "metadata": {
        "id": "X7PwROtfCwq_"
      }
    }
  ]
}